services:
  training:
    build:
      context: .
      dockerfile: Dockerfile
    # This command first runs the one-time preprocessing, then starts training.
    # The preprocessing script is smart enough to skip if the output file already exists.
    command: sh -c "python src/preprocess_data.py && python src/train.py"
    volumes:
      - ./src:/app/src
      - model-data:/app/results
      - data-cache:/app/data # Persists preprocessed data
    environment:
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  webapp:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      training:
        condition: service_completed_successfully
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - ./static:/app/static
      - ./templates:/app/templates
      - model-data:/app/results:ro
    environment:
      - PYTHONPATH=/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# Define the named volumes that will be shared between services.
volumes:
  model-data:
    driver: local
  data-cache:
    driver: local