version: "3.9"

services:
  training:
    build:
      context: .
      # Use the specific Dockerfile for GPU
      dockerfile: Dockerfile.gpu
    command: python src/train.py
    volumes:
      - ./src:/app/src
      - model-data:/app/results
    deploy:
      resources:
        reservations:
          devices:
            # This section grants the container access to the host's GPU
            - driver: nvidia
              count: all # or specify a number, e.g., 1
              capabilities: [gpu]

  webapp:
    build:
      context: .
      # Also use the GPU Dockerfile for the webapp for faster inference
      dockerfile: Dockerfile.gpu
    depends_on:
      training:
        condition: service_completed_successfully
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - ./static:/app/static
      - ./templates:/app/templates
      - model-data:/app/results:ro
    deploy:
      resources:
        reservations:
          devices:
            # Also give the webapp access to the GPU
            - driver: nvidia
              count: all
              capabilities: [gpu]

# Define the named volume that will be shared between services.
volumes:
  model-data:
    driver: local