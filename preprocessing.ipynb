{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets pandas spacy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45872ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"trl-lib/tldr\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38032dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_prompt(example):\n",
    "    text = example[\"prompt\"]\n",
    "    # Regex with DOTALL so that POST: can span multiple lines\n",
    "    m = re.match(\n",
    "        r\"SUBREDDIT:\\s*(?P<subreddit>.+?)\\s+TITLE:\\s*(?P<title>.+?)\\s+POST:\\s*(?P<post>.+?)\\s+TL;DR:\",\n",
    "        text,\n",
    "        flags=re.DOTALL,\n",
    "    )\n",
    "    if not m:\n",
    "        return {\"subreddit\": None, \"title\": None, \"post\": text}\n",
    "    return m.groupdict()\n",
    "\n",
    "ds = ds.map(split_prompt, remove_columns=[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d436fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‘completion’ → ‘tldr’, drop any rows missing required fields\n",
    "ds = ds.rename_column(\"completion\", \"tldr\")\n",
    "ds = ds.filter(lambda x: x[\"subreddit\"] and x[\"title\"] and x[\"post\"] and x[\"tldr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959700aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def clean_text(example):\n",
    "    for col in [\"title\", \"post\", \"tldr\"]:\n",
    "        text = example[col].strip()\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        example[col] = text\n",
    "    return example\n",
    "\n",
    "ds = ds.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def compute_similar(example):\n",
    "    doc_post = nlp(example[\"post\"])\n",
    "    doc_tldr = nlp(example[\"tldr\"])\n",
    "    tldr_text = example[\"tldr\"].lower()\n",
    "    similar = {}\n",
    "    for chunk in doc_post.noun_chunks:\n",
    "        phrase = chunk.text.strip().lower()\n",
    "        if len(phrase) < 3:\n",
    "            continue\n",
    "        # Binary importance if phrase appears in the TL;DR summary\n",
    "        important = 1 if phrase in tldr_text else 0\n",
    "        # Similarity score via spaCy vectors\n",
    "        sim_score = float(doc_tldr.similarity(nlp(phrase)))\n",
    "        if sim_score >= 0.75: # Threshold for similarity\n",
    "            important = 1\n",
    "        similar[phrase] = (important, sim_score)\n",
    "    return {\"similar\": similar}\n",
    "\n",
    "# Apply with a progress bar\n",
    "records = []\n",
    "for row in tqdm(ds, total=len(ds)):\n",
    "    rec = dict(row)\n",
    "    rec.update(compute_similar(row))\n",
    "    records.append(rec)\n",
    "\n",
    "# Convert back into a Dataset\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(pd.DataFrame(records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_top_k(example, k: int = 30):\n",
    "    items = sorted(\n",
    "        example[\"similar\"].items(),\n",
    "        key=lambda kv: kv[1][1],\n",
    "        reverse=True\n",
    "    )[:k]\n",
    "    example[\"similar\"] = dict(items)\n",
    "    return example\n",
    "\n",
    "ds = ds.map(keep_top_k)\n",
    "\n",
    "ds = ds.filter(lambda x: len(x[\"similar\"]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the Dataset to a pandas DataFrame\n",
    "df = ds.to_pandas()\n",
    "\n",
    "# Serialize the nested `similar` dict into JSON strings\n",
    "df['similar'] = df['similar'].apply(json.dumps)\n",
    "\n",
    "# Write out to CSV (no index column)\n",
    "df.to_csv(\"tldr_preprocessed.csv\", index=False)\n",
    "\n",
    "# loading the preprocessed data later:\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"tldr_preprocessed.csv\")\n",
    "# df['similar'] = df['similar'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37355a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
